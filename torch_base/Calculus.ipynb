{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 微积分\n",
    "\n",
    "#### 微分\n",
    "模型训练主要是解决优化问题。解决优化问题可以用到微积分。\n",
    "优化问题的核心是 “寻找函数的最值（最大值或最小值）”—— 无论是数学中的函数极值求解，还是实际场景中的 “成本最低、利润最高、效率最优”，微分都通过刻画函数的 “局部变化趋势”，为最值的定位提供了严格的数学依据。\n",
    "微分解决优化问题的本质是利用 “函数在极值点处的导数为零（或导数不存在）” 这一核心性质，通过三步完成最值求解：\n",
    "1. 将实际问题转化为目标函数\n",
    "先明确优化的 “目标”（如利润、成本、体积等），再将其表示为单一变量的函数 `y=f(x)`，比如某厂商生产某产品，成本函数为\n",
    " $$\n",
    "\\begin{aligned}\n",
    "C(x) &= 2x^2+3x+100\n",
    "\\end{aligned}\n",
    "$$\n",
    "以上x表示产量，利润函数为\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(x) &= 20x-C(x) &= -2x^2 + 17x + 100\n",
    "\\end{aligned}\n",
    "$$\n",
    "把求最大利润问题转化为求P(x)最大值的问题。\n",
    "2. 用导数找到可能的极值点\n",
    "对目标函数求导，导数为\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{dP}{dx} &= -4x + 17\n",
    "\\end{aligned}\n",
    "$$\n",
    "零的导数函数的值为0，即\n",
    "$$\n",
    "\\begin{aligned}\n",
    "-4x + 17 &= 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "这里求出的x的值是可能为极值的点，专业来说为驻点。什么意思？导数实际上反应函数的变化率。一般极值点的导数函数值为0，也就是变化率为0，不变化了就类似于停住了，就是驻点.当然一个函数的形状可能千奇百怪，所以导数为0的点也不一定是极值点。上面我们求出来的值为4.25，表示在产量为4.25时，可能利润为最大值。\n",
    "3. 判定极值点\n",
    "\n",
    "有两种方法，一种是一阶导数的符号判断，另一种通过二阶导数符号法判断。\n",
    "\n",
    "一阶导数符号法为：分析驻点左右两侧导数值的符号，如果左正右负，则该点为极大值点（函数先增后减，此处为峰值）；如果左负右正，则该点为极小值点（函数先减后增，此处为谷值）。\n",
    "\n",
    "比如以上P(x)的一阶导数为：P'(x) = -4x + 17，又因为x表示产量，所以我们可以把x的取值区间分为两个[0,4.25], [4.25, +∞)。\n",
    "从左侧随便找一个点，比如x=4, P'(x) = -4x + 17 = 1 > 0，右侧随便找一个点，比如x=5, P'(x) = -4x + 17 = -3 < 0，左正右负，所以该点为极大值点。\n",
    "\n",
    "二阶导数符号法为：求二阶导数，带入驻点x0，如果二阶导数大于0，则该点为极小值点；如果二阶导数小于0，则该点为极大值点。\n",
    "函数P(x)的二阶导数为：P''(x) = -4 ，P''(4) = -4 > 0，该点为极小值点。\n"
   ],
   "id": "f080fb177ee500cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 积分\n",
    "微分是把 “整体” 拆成 “局部小量”（比如把曲线拆成无数小段直线，用 “瞬时变化率” 描述某一点的趋势），而积分则是把 “无数局部小量” 重新 “累积还原成整体”—— 它解决的是 “如何通过局部变化规律，计算出整体的总量、总面积、总效应” 等问题。\n",
    "\n",
    "| 类型     | 核心定位                    | 解决的问题（通俗理解）                                       | 类比微分（参考）              |\n",
    "| -------- | --------------------------- | ------------------------------------------------------------ | ----------------------------- |\n",
    "| 不定积分 | 微分的 “逆运算”（求原函数） | 已知 “变化率函数”，求 “原总量函数”（比如已知速度，求位移函数） | 微分是 “已知位移，求速度”     |\n",
    "| 定积分   | 局部小量的 “累积求和”       | 已知 “变化率函数”，求 “某区间内的具体总量”（比如已知速度，求 1 小时内的总位移） | 微分是 “求某一时刻的瞬时速度” |\n",
    "\n",
    "比如求函数 y=x^2、x 轴、直线 x=0和 x=2围成的 “曲边梯形” 面积\n",
    "\n",
    "我们无法直接用矩形面积公式计算（因为上边是曲线，不是直线），但可以把这个梯形 “拆成无数个窄矩形”（每个矩形的宽度是微小的 Δx，高度是该点的函数值 f(x)），每个窄矩形的面积就是 “局部小量” ΔS≈f(x)⋅Δx（这一步其实是微分近似：用矩形面积近似小曲边梯形面积）。\n",
    "\n",
    "梯形面积 = 宽Δx/2 * (f(x0) + f(x1) + f(x2) + ... + f(xn))\n",
    "\n",
    "把这 “无数个窄矩形的面积” 全部加起来，再让每个矩形的宽度 Δx无限趋近于 0（消除近似误差），最终得到的精确总和，就是这个曲边梯形的面积 —— 这就是定积分的本质：S=∫f(x)dx\n",
    "这里的 ∫符号原本就是 “求和（Sum）” 的缩写，下标 0 和上标 2 表示 “累积的区间”，x^2dx就是 “局部小面积的微分形式”。\n",
    "\n"
   ],
   "id": "5299eec1b58875e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 常见函数的导数\n",
    "常见函数的导数，D为导数符号\n",
    "1. DC=0(C是常数)\n",
    "2. Dx^n=nx^(n-1)\n",
    "3. De^x=e^x\n",
    "4. Dln(x)=1/x"
   ],
   "id": "48eee57166cf3e8a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 常见函数组成的符合函数的微分\n",
    "- 常数乘法则\n",
    "\n",
    "假设函数可微，C为常数，则：\n",
    "\n",
    "$$D(Cf(x))=Cf'(x)$$\n",
    "\n",
    "- 乘法法则\n",
    "\n",
    "假设函数可微，则：\n",
    "\n",
    "$$D(f(x)g(x))=f'(x)g(x)+f(x)g'(x)$$\n",
    "\n",
    "\n",
    "- 加法法则\n",
    "\n",
    "假设函数可微，则：\n",
    "\n",
    "$$D(f(x)+g(x))=f'(x)+g'(x)$$\n",
    "\n",
    "- 除法法则\n",
    "\n",
    "假设函数可微，则：\n",
    "\n",
    "$$D\\left(\\frac{f(x)}{g(x)}\\right)=\\frac{f'(x)g(x)-f(x)g'(x)}{g(x)^2}$$\n"
   ],
   "id": "bb649a59cbe7b131"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 偏导数\n",
    "以上我们是对一元函数的一些处理，但是对于多元函数，比如：$f(x,y)$，$f(x,y,z)$，$f(x_1,x_2,...,x_n)$，就需要引入偏导数的概念。\n",
    "偏导数，也叫偏导，是关于某一个变量的导数。比如：\n",
    "\n",
    "$$f(x,y)=x^2+y^2$$\n",
    "\n",
    "对于$f(x,y)$，$x$和$y$都是变量，$x$和$y$都是函数的输入参数，$f(x,y)$是函数的输出结果。\n",
    "\n",
    "假设f(x)，是含有N个变量的函数，那么f(x)，关于第i个参数的偏导数可以定义为：\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_i} = \\lim_{h \\to 0} \\frac{f(x_1,x_2,...,x_{i-1},x_i+h,x_{i+1},...,x_N)-f(x_1,x_2,...,x_{i-1},x_i,x_{i+1},...,x_N)}{h}\n",
    "$$\n",
    "以上公式可以写为：\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_i}\n",
    "$$\n",
    "\n",
    "计算时，把x1，x2，...，xN，分别设为x，y，...，z，也就看作常数那么：\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x_i} = \\lim_{h \\to 0} \\frac{f(x,y,...,z,x_i+h)-f(x,y,...,z,x_i)}{h}\n",
    "$$\n",
    "\n",
    "偏导数有如下表达形式：\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial f}{\\partial x_i}= f_{x_i}=D_{x_i}f=D_if\n",
    "$$\n",
    "\n"
   ],
   "id": "a4118d1b13baac97"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 梯度\n",
    "对于多元函数，我们可以把其对所有自变量的偏导数组合成一个向量，这就是梯度向量。\n",
    "\n",
    "假设函数为：\n",
    "$$\n",
    "f(x,y,z)=x^2+y^2+z^2\n",
    "$$\n",
    "梯度向量为：\n",
    "$$\n",
    "\\nabla f = \\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x}\\\\\n",
    "\\frac{\\partial f}{\\partial y}\\\\\n",
    "\\frac{\\partial f}{\\partial z}\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "2x\\\\\n",
    "2y\\\\\n",
    "2z\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n"
   ],
   "id": "1df8a5b4f16c922b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 链式法则\n",
    "单变量函数的链式法则，如果y=f(u)且u=g(x)均可微分，则：\n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dx}\n",
    "$$\n",
    "\n",
    " 多变量函数的链式法则，如果y=f(u,v)且u=g(x,y)且v=h(x,y)均可微分，则：\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial u} \\frac{\\partial u}{\\partial x} + \\frac{\\partial y}{\\partial v} \\frac{\\partial v}{\\partial x}\n",
    "$$"
   ],
   "id": "46b7ec3edfca4f47"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 自动微分\n",
    "求导几乎是所有深度学习优化算法的核心步骤。单个求导的计算很简单，但是复杂模型手工求导并更新参数是一件十分复杂的事情，几乎不可能手动完成。所以pytorch引入了自动求导。怎么实现的呢？\n",
    "- 构建计算图：将所有计算过程都记录下来，并保存在计算图中。\n",
    "- 反向传播：根据计算图，计算梯度。\n",
    "- 优化参数：根据梯度，更新参数。\n",
    "\n",
    "计算过程：\n",
    "假设有一个函数，是f(x,y)=(x+y)*x，吗，目标是求f对x和y的偏导数∂f/∂x, ∂f/∂y\n",
    "1. 拆函数，把复杂的函数拆分成简单的步骤\n",
    "\n",
    "把f函数拆成三个节点\n",
    "\n",
    "- 输入节点：输入x,y\n",
    "- 中间节点1：a=x+y\n",
    "- 中间节点2：y=a*x\n",
    "\n",
    "2. 画出计算图\n",
    "\n",
    "```plantext\n",
    "x ──┬──→ [+] → a ──┬──→ [×] → f\n",
    "    │               │\n",
    "y ──┘               │\n",
    "                    │\n",
    "x ──────────────────┘\n",
    "```\n",
    "\n",
    "3. 正向计算，填充每个节点的具体数值\n",
    "\n",
    "给x和y赋值，比如x=3,y=4\n",
    "\n",
    "- 输入节点：x=3,y=4\n",
    "- 中间节点1：a=x+y=3+4=7\n",
    "- 中间节点2：y=a*x=7*3=21\n",
    "- 输出节点：f=y=21\n",
    "\n",
    "4. 反向传播，基于链式法则传梯度\n",
    "\n",
    "梯度就是求导，反向传播就是基于f向前算，每个节点的梯度=上游传统过来的梯度*自己的局部梯度\n",
    "\n",
    "- f节点的梯度（对自己的梯度）：dy/df=1\n",
    "- 中间节点a的梯度：f的梯度*a的局部梯度， 局部梯度就是说x固定不变，a增加对f的影响有多大，也就是说f值对a求导数，那么结果就是x，在我们的例子里，x=3,则中间节点a对f的梯度就是3，则a的梯度就是1*3=3\n",
    "- x的梯度：计算x的梯度需要看x对下游节点有几个直接影响，在我们的例子里有两个，一个是对a，一个是对f，所以要分别计算出来后把梯度加起来就是x的梯度\n",
    "\n",
    "    -- 首先计算x对a的梯度，我们固定y=4， x对a的梯度为δa/δx * 传过来的a的梯度=1*3=3\n",
    "    -- 然以计算x对f的梯度，固定x+y=3+4=7, 则x对f的梯度是δf/δx * 传过来的f的梯度=7*1=7\n",
    "    -- 所以x的梯度是3+7=10\n",
    "\n",
    "- y的梯度：y和a有直接关系，y对a的梯度是δa/δy * 传过来的a的梯度=1*3=3\n",
    "\n",
    "\n"
   ],
   "id": "d3834fc37ab0e321"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T08:10:08.504556Z",
     "start_time": "2025-09-02T08:10:08.500501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Pytorch的梯度向量自动保存\n",
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "x.requires_grad_(True) # 开辟内存空间存储梯度\n",
    "y = 2 * torch.dot(x, x) # 点积 对应位置相乘，结果相加\n",
    "y.backward()  # 反向传播\n",
    "print(x.grad)  # 获取梯度，梯度会更新，保存在开辟的空间中"
   ],
   "id": "4bade00d7ed75fbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.])\n",
      "tensor(28., grad_fn=<MulBackward0>)\n",
      "tensor([ 0.,  4.,  8., 12.])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T08:14:49.505452Z",
     "start_time": "2025-09-02T08:14:49.502245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 重新计算之间要清零,不然会累加\n",
    "x.grad.zero_()\n",
    "y = 4*torch.dot(x, x)\n",
    "y.backward()\n",
    "print(x.grad)"
   ],
   "id": "f084e6f098bea9b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  8., 16., 24.])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 练习\n",
    "如果存在如下关系：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_1=x_1 * x_2 * x_3 \\\\\n",
    "y_2 = x_1 + x_2 + x_3 \\\\\n",
    "y_3 = x_1 + x_2 * x_3 \\\\\n",
    "A = f(y_1, y_2, y_3) \\\\\n",
    "其中f(y_1, y_2, y_3)形式未知，求 \\\\\n",
    "\\frac{\\partial A}{\\partial x_1},\n",
    "\\frac{\\partial A}{\\partial x_2},\n",
    "\\frac{\\partial A}{\\partial x_3}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "首先y是有关x的函数，而A是有关y的函数，根据链式法则，可以得到：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial A}{\\partial x_1} = \\frac{\\partial A}{\\partial y_1} \\frac{\\partial y_1}{\\partial x_1} + \\frac{\\partial A}{\\partial y_2} \\frac{\\partial y_2}{\\partial x_1} + \\frac{\\partial A}{\\partial y_3} \\frac{\\partial y_3}{\\partial x_1} \\\\\n",
    "\\frac{\\partial A}{\\partial x_2} = \\frac{\\partial A}{\\partial y_1} \\frac{\\partial y_1}{\\partial x_2} + \\frac{\\partial A}{\\partial y_2} \\frac{\\partial y_2}{\\partial x_2} + \\frac{\\partial A}{\\partial y_3} \\frac{\\partial y_3}{\\partial x_2} \\\\\n",
    "\\frac{\\partial A}{\\partial x_3} = \\frac{\\partial A}{\\partial y_1} \\frac{\\partial y_1}{\\partial x_3} + \\frac{\\partial A}{\\partial y_2} \\frac{\\partial y_2}{\\partial x_3} + \\frac{\\partial A}{\\partial y_3} \\frac{\\partial y_3}{\\partial x_3}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "以上计算可以简化为：\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial A}{\\partial x_1} \\\\\n",
    "\\frac{\\partial A}{\\partial x_2} \\\\\n",
    "\\frac{\\partial A}{\\partial x_3}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_3}{\\partial x_1} \\\\\n",
    "\\frac{\\partial y_1}{\\partial x_2} & \\frac{\\partial y_2}{\\partial x_2} & \\frac{\\partial y_3}{\\partial x_2} \\\\\n",
    "\\frac{\\partial y_1}{\\partial x_3} & \\frac{\\partial y_2}{\\partial x_3} & \\frac{\\partial y_3}{\\partial x_3}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial A}{\\partial y_1} \\\\\n",
    "\\frac{\\partial A}{\\partial y_2} \\\\\n",
    "\\frac{\\partial A}{\\partial y_3}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "根据已知的y和x的关系，我们可以计算出雅可比矩阵的各个元素：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial y_1}{\\partial x_1} &= x_2 x_3, \\quad\n",
    "\\frac{\\partial y_1}{\\partial x_2} = x_1 x_3, \\quad\n",
    "\\frac{\\partial y_1}{\\partial x_3} = x_1 x_2 \\\\\n",
    "\\frac{\\partial y_2}{\\partial x_1} &= 1, \\quad\n",
    "\\frac{\\partial y_2}{\\partial x_2} = 1, \\quad\n",
    "\\frac{\\partial y_2}{\\partial x_3} = 1 \\\\\n",
    "\\frac{\\partial y_3}{\\partial x_1} &= 1, \\quad\n",
    "\\frac{\\partial y_3}{\\partial x_2} = x_3, \\quad\n",
    "\\frac{\\partial y_3}{\\partial x_3} = x_2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "将这些偏导数代入矩阵，得到：\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial A}{\\partial x_1} \\\\\n",
    "\\frac{\\partial A}{\\partial x_2} \\\\\n",
    "\\frac{\\partial A}{\\partial x_3}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_2 x_3 & 1 & 1 \\\\\n",
    "x_1 x_3 & 1 & x_3 \\\\\n",
    "x_1 x_2 & 1 & x_2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial A}{\\partial y_1} \\\\\n",
    "\\frac{\\partial A}{\\partial y_2} \\\\\n",
    "\\frac{\\partial A}{\\partial y_3}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "传入具体数值：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_1=1, \\quad\n",
    "x_2=2, \\quad\n",
    "x_3=3\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "根据x的值，得到雅可比矩阵为：\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "6 & 1 & 1 \\\\\n",
    "3 & 1 & 3 \\\\\n",
    "2 & 1 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "假设已知 $\\frac{\\partial A}{\\partial y_1} = 0.1$, $\\frac{\\partial A}{\\partial y_2} = 0.2$, $\\frac{\\partial A}{\\partial y_3} = 0.3$，即 $\\nabla_y A = \\begin{bmatrix} 0.1 \\\\ 0.2 \\\\ 0.3 \\end{bmatrix}$\n",
    "\n",
    "最终运算结果为：\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial A}{\\partial x_1} \\\\\n",
    "\\frac{\\partial A}{\\partial x_2} \\\\\n",
    "\\frac{\\partial A}{\\partial x_3}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "6 & 1 & 1 \\\\\n",
    "3 & 1 & 3 \\\\\n",
    "2 & 1 & 2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\\n",
    "0.2 \\\\\n",
    "0.3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "6 \\times 0.1 + 1 \\times 0.2 + 1 \\times 0.3 \\\\\n",
    "3 \\times 0.1 + 1 \\times 0.2 + 3 \\times 0.3 \\\\\n",
    "2 \\times 0.1 + 1 \\times 0.2 + 2 \\times 0.3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1.1 \\\\\n",
    "1.4 \\\\\n",
    "1.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "因此：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial A}{\\partial x_1} &= 1.1 \\\\\n",
    "\\frac{\\partial A}{\\partial x_2} &= 1.4 \\\\\n",
    "\\frac{\\partial A}{\\partial x_3} &= 1.0\n",
    "\\end{aligned}\n",
    "$$\n"
   ],
   "id": "f77533cbf877a154"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T03:46:22.818622Z",
     "start_time": "2025-09-03T03:46:22.809717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 以上计算用pytorch实现\n",
    "import torch\n",
    "x1 = torch.tensor(1, requires_grad=True, dtype=torch.float32)\n",
    "x2 = torch.tensor(2, requires_grad=True, dtype=torch.float32)\n",
    "x3 = torch.tensor(3, requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "# 计算y值\n",
    "y1 = x1 * x2 * x3\n",
    "y2 = x1 + x2 + x3\n",
    "y3 = x1 + x2 * x3\n",
    "\n",
    "# 组合成y向量\n",
    "y = torch.stack([y1, y2, y3])\n",
    "y.backward(torch.tensor([0.1, 0.2, 0.3], dtype=torch.float32))\n",
    "print(x1.grad, x2.grad, x3.grad)"
   ],
   "id": "60ff919e9b04cec1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1000) tensor(1.4000) tensor(1.)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 分离计算\n",
    "有时候我们想把计算图中的某些梯度隐去，比如某些参数不需要梯度，或者某些参数的梯度需要重新计算。"
   ],
   "id": "c6cf482959d21e0f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T04:01:14.675151Z",
     "start_time": "2025-09-03T04:01:14.669884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 比如,忽略y，只计算z对x的梯度\n",
    "import torch\n",
    "x = torch.arange(4, requires_grad=True, dtype=torch.float32)\n",
    "x.requires_grad_()\n",
    "y = x*x\n",
    "u=y.detach()  # 分离y对x的计算结果\n",
    "z = u*x\n",
    "z.sum().backward()\n",
    "print(x.grad)\n",
    "print(u)"
   ],
   "id": "afaab5684b049acb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 4., 9.])\n",
      "tensor([0., 1., 4., 9.])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-03T04:05:00.846024Z",
     "start_time": "2025-09-03T04:05:00.841430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### python控制流的梯度也可以进行计算\n",
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.sum() > 1000:\n",
    "        b = b * 0.5\n",
    "    return b\n",
    "a = torch.randn(size=(3,), requires_grad=True)\n",
    "b = f(a)\n",
    "b.sum().backward()\n",
    "print(a.grad)"
   ],
   "id": "888d7bb19344f79c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2.])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d2818023ce974cdb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
